N-Gram Text Generator
======================

Overview
--------
This project implements an n-gram language model to generate text using Markov chains. The model supports three tokenization methods: word-level, character-level, and subword-level. Text is generated for n-grams of size 2 (bigram), 3 (trigram), and 4 (4-gram). The results illustrate how Markov chains model text and how tokenization and context size affect the generated output.

Training Text
-------------
The model was trained on the full text of "Moby Dick" by Herman Melville, a classic novel that provides a rich dataset for modeling English language patterns.

How It Works
------------
1. The input text is tokenized into words, characters, or subwords.
2. N-gram models are built by counting the occurrences of sequences (bigrams, trigrams, and 4-grams).
3. Text is generated by sampling the most likely next token based on the n-gram model.
4. A fallback mechanism ensures that if higher-order n-grams (e.g., 4-grams) lack context matches, the model uses lower-order n-grams (e.g., trigrams or bigrams).

Example Sentences
-----------------
Below are example outputs for each tokenization type and n-gram size. Sentences are truncated to a maximum of three lines if necessary.

Word Tokenization
-----------------
Bigram:
Call me Ishmael . Some years ago never mind how long precisely . Having little or no money in my purse and nothing 
particular to the Mediterranean , bumped the head out brimmers all dangerous to the throne somewhere out of some nearer 
to a score of panic did he had been , like his foe were in a fool saved .

Trigram:
Call me Ishmael . Some years ago never mind how long precisely . Having little or no money in my purse and nothing 
particular to interest me on shore , as this Leviathan , that his bones to quiver in him , attentively , and the round 
again , and without using any words was meanwhile lowly humming to himself out for new stars .

4-Gram:
Call me Ishmael . Some years ago never mind how long precisely . Having little or no money in my purse and nothing 
particular to prove it .

Character Tokenization
----------------------
Bigram:
C a l l   m e   I s h m a e l .   S o m e   y e a r s   a g o ,   n e v e r   m i n d   h o w   l o n g   p r e c i s e l y .

Trigram:
C a l l   m e   I s h m a e l .   S o m e   y e a r s   a g o ,   n e v e r   m i n d   h o w   l o n g   p r e c 
i s e l y .

4-Gram:
C a l l   m e   I s h m a e l .   S o m e   y e a r s   a g o ,   n e v e r   m i n d   h o w   l o n g   p r e c 
i s e l y .

Subword Tokenization
---------------------
Bigram:
Call me Ishmael. Some years ago, never mind how long precisely. The course of uncertain like edge gauntless, I had clanged 
then, stripped of orphans, after bulwondiberated Protestant of side next instant, then Gemini himself out abroad upon the 
ter had twice as he.

Trigram:
Call me Ishmael. Some years ago, never mind how long precisely. The allusion to the sperm whale could, by many nations and 
theyâ€™t I tell ye what it will answer a single, ever were. And all from looking at his maternal sea.

4-Gram:
Call me Ishmael. Some years ago, never mind how long precisely.

Techniques Used
---------------
1. **N-Gram Fallback**:
   - If higher-order n-grams (e.g., trigrams, 4-grams) lack matches for a given context, the model falls back to lower-order n-grams (e.g., bigrams) to continue generation. This ensures that text generation does not fail due to missing context matches.

2. **Subword Tokenization**:
   - Subword tokenization is implemented using GPT-2's tokenizer (`tiktoken`). Subwords allow the model to handle out-of-vocabulary words or rare word patterns by breaking them into smaller, meaningful units.

3. **Dynamic Tokenization**:
   - Three different tokenization methods (word, character, and subword) are used, each demonstrating unique advantages and challenges. 
     - **Word Tokenization** focuses on word-level semantics.
     - **Character Tokenization** generates sequences at the character level, enabling creative outputs but often leading to less coherence.
     - **Subword Tokenization** balances vocabulary size and linguistic representation.

4. **Start Tokens**:
   - Manually defined starting tokens were used to provide meaningful context for generation in each tokenization type.

Key Observations
----------------
1. **Higher-Order N-Grams**:
   - Trigrams and 4-grams produce more coherent and contextually relevant sentences compared to bigrams, which often result in disjointed sequences.

2. **Tokenization Effects**:
   - Word-level tokenization generates the most human-readable outputs.
   - Character-level tokenization captures syntax but struggles with semantics.
   - Subword-level tokenization is compact and effective but sometimes generates unnatural sequences due to token splits.

3. **Training Corpus**:
   - The quality of generated text is highly dependent on the richness and structure of the training text. Using "Moby Dick" allowed the model to learn from complex sentence structures and varied vocabulary.

Usage
-----
1. Place the `Moby_Dick.txt` file in the same directory as the script.
2. Run the script to generate example sentences for all tokenization types and n-gram sizes.
3. The results will be printed in the console and can be further analyzed.

